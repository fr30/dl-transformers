{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import preprocess_tinyshakespare\n",
    "\n",
    "preprocess_tinyshakespare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de2e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/VSCodeProjects/dl-transformers/src/dataset.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.tokens = torch.load(f\"{path}_tokens.pt\")\n",
      "/home/kuba/VSCodeProjects/dl-transformers/src/dataset.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.labels = torch.load(f\"{path}_labels.pt\")\n",
      "/home/kuba/VSCodeProjects/dl-transformers/src/dataset.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.offsets = torch.load(f\"{path}_offsets.pt\")\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import TinyShakespeareDataset\n",
    "    \n",
    "\n",
    "train_ds = TinyShakespeareDataset('train')\n",
    "val_ds = TinyShakespeareDataset('val')\n",
    "test_ds = TinyShakespeareDataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "class FFLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        dff = emb_dim * 8 // 3\n",
    "        self.W1 = nn.Linear(emb_dim, dff)\n",
    "        self.W2 = nn.Linear(dff, emb_dim)\n",
    "        self.W3 = nn.Linear(emb_dim, dff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.silu(self.W1(x))\n",
    "        y = y * self.W3(x)\n",
    "        y = self.W2(y)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def silu(x):\n",
    "        return x * F.sigmoid(x)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_size, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(emb_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_dtype = x.dtype\n",
    "        rms = torch.sqrt(torch.mean(x.pow(2)) + self.eps)\n",
    "        x = (x / rms) * self.g\n",
    "        return x.to(in_dtype)\n",
    "\n",
    "class CausalSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=512, max_seq_len=1050, n_heads=8, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        # self.Wq = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        # self.Wk = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        # self.Wv = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        self.W = nn.Linear(input_dim, emb_dim * 3, bias=True)\n",
    "        self.Wo = nn.Linear(emb_dim, emb_dim)\n",
    "        self.ff = FFLayer(emb_dim)\n",
    "        self.ln_mha = RMSNorm(emb_dim)\n",
    "        self.ln_ff = RMSNorm(emb_dim)        \n",
    "        self.scale = 1 / np.sqrt(emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        attn_mask = torch.tril(torch.ones(max_seq_len, max_seq_len)) == 0\n",
    "        self.register_buffer(\"_causal_mask\", attn_mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.mha(x)\n",
    "        y_prenorm = y + x\n",
    "\n",
    "        y = self.mlp(y_prenorm)\n",
    "        y = y_prenorm + y\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        y = self.ln_ff(x)\n",
    "        y = self.ff(y)\n",
    "        y = F.dropout(y, p=self.p_dropout)\n",
    "    \n",
    "        return y\n",
    "    \n",
    "    def mha(self, x):\n",
    "        b, t, e = x.shape\n",
    "        s = e // self.n_heads\n",
    "        \n",
    "        # x = self.ln_mha(x)\n",
    "        # q, k, v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        # q = q.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "        # k = k.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "        # v = v.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "\n",
    "        x = self.ln_mha(x)\n",
    "        qkv = self.W(x)\n",
    "        qkv = qkv.view(b, t, self.n_heads, 3 * s)\n",
    "        qkv = qkv.transpose(1, 2)\n",
    "        q, k, v = qkv.split(s, dim=-1)\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) * self.scale\n",
    "        attn = attn.masked_fill(self._causal_mask[:t, :t], -torch.inf)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = F.dropout(attn, p=self.p_dropout)\n",
    "        y = attn @ v\n",
    "        y = y.transpose(-2, -3).reshape(b, t, e)\n",
    "        y = self.Wo(y)\n",
    "        y = F.dropout(y, p=self.p_dropout)\n",
    "\n",
    "        return y\n",
    "\n",
    "        \n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size=8124, \n",
    "            emb_dim=512, \n",
    "            attn_blocks=4, \n",
    "            max_seq_len=1024, \n",
    "            n_heads=16, \n",
    "            p_dropout=0.1, \n",
    "            tokenizer_path=\"tokenizer.json\"\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.attn = nn.Sequential(\n",
    "            *[CausalSelfAttentionBlock(emb_dim, emb_dim, max_seq_len, n_heads, p_dropout) for _ in range(attn_blocks)]\n",
    "        )\n",
    "        self.mlp = nn.Linear(emb_dim, vocab_size)\n",
    "        self.layer_norm = RMSNorm(emb_dim)\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        self.register_buffer(\"_device_tracker\", torch.empty(0))\n",
    "        pe = self._compute_pe(max_seq_len, emb_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device_tracker.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, t = x.shape\n",
    "        x = self.emb(x)\n",
    "        x = x + self.pe[:, :t, :]\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.attn(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def run_inference(self, text_input, tau=1.0, k=10):\n",
    "        x = self.tokenizer.encode(text_input).ids[:-1]\n",
    "        eos_id = self.tokenizer.token_to_id(\"<eos>\")\n",
    "        next_token = -1\n",
    "        cur_iter = 0\n",
    "        max_iter = 128\n",
    "\n",
    "        while (next_token != eos_id and cur_iter < max_iter):\n",
    "            x_tensor = torch.tensor(x).to(self.device)\n",
    "            logits = self.forward(x_tensor.unsqueeze(0))\n",
    "            q = F.softmax(logits / tau, dim=-1)[:, -1]\n",
    "            topk = q.topk(k=k)\n",
    "            next_token_index = topk.values.multinomial(1).item()\n",
    "            next_token = topk.indices[0, next_token_index]\n",
    "            x += [next_token]\n",
    "            cur_iter += 1\n",
    "        \n",
    "        return self.tokenizer.decode(x)\n",
    "        \n",
    "    def _compute_pe(self, max_seq_len, emb_dim):\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-np.log(10000.0) / emb_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) \n",
    "\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff833460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class GPT2LRScheduler:\n",
    "    def __init__(self, optim, max_lr=1e-3, min_lr=3e-5, warmup_steps=2000, total_steps=200000):\n",
    "        self.optim = optim\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.warmup_steps = max(1, int(warmup_steps))\n",
    "        self.total_steps = max(self.warmup_steps + 1, int(total_steps))\n",
    "        self.step = 0\n",
    "\n",
    "    def adjust_lr(self):\n",
    "        self.step += 1\n",
    "        s = self.step\n",
    "\n",
    "        if s <= self.warmup_steps:\n",
    "            # linear warmup: 0 -> max_lr\n",
    "            lr = self.max_lr * (s / self.warmup_steps)\n",
    "        else:\n",
    "            # cosine decay: max_lr -> min_lr\n",
    "            progress = (s - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            progress = min(max(progress, 0.0), 1.0)\n",
    "            cosine = 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * cosine\n",
    "\n",
    "        for pg in self.optim.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34db60fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "DataParallel                                  --\n",
      "├─NanoGPT: 1-1                                --\n",
      "│    └─Embedding: 2-1                         4,159,488\n",
      "│    └─Sequential: 2-2                        --\n",
      "│    │    └─CausalSelfAttentionBlock: 3-1     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-2     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-3     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-4     3,151,530\n",
      "│    └─Linear: 2-3                            4,167,612\n",
      "│    └─RMSNorm: 2-4                           512\n",
      "======================================================================\n",
      "Total params: 20,933,732\n",
      "Trainable params: 20,933,732\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:12<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 5.497071646027646, Val Loss: 4.470759391784668, Val Perplexity: 87.42308618586773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 4.380050853147345, Val Loss: 4.115286350250244, Val Perplexity: 61.269756361106374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 4.031834772077658, Val Loss: 3.9667584896087646, Val Perplexity: 52.81305899640893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 3.714634656906128, Val Loss: 3.9460620880126953, Val Perplexity: 51.731252087931566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 3.4584456985279663, Val Loss: 3.938840389251709, Val Perplexity: 51.35901029565926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 3.300766977213197, Val Loss: 3.9603688716888428, Val Perplexity: 52.47667954079065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 3.0874060089305297, Val Loss: 4.019615650177002, Val Perplexity: 55.67970123032022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 3.0891660189224504, Val Loss: 4.021398544311523, Val Perplexity: 55.7790607905246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 2.9379237910448492, Val Loss: 4.0247979164123535, Val Perplexity: 55.96899722298665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:11<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 2.888797848911609, Val Loss: 4.046222686767578, Val Perplexity: 57.18105783254657\n",
      "Test Loss: 4.70983007975987, Test Perplexity: 111.03329149912854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def clip_gradient(grad):\n",
    "#     max_grad_norm = \n",
    "\n",
    "def collate_batch_fn(args, pad_token):\n",
    "    x = [t[0] for t in args]\n",
    "    y = [t[1] for t in args]\n",
    "    xlen = len(x)\n",
    "    padded_xy = pad_sequence(x + y, padding_value=pad_token, batch_first=True)\n",
    "    padded_x = padded_xy[:xlen]\n",
    "    padded_y = padded_xy[xlen:]\n",
    "\n",
    "    return padded_x, padded_y\n",
    "\n",
    "lrs = []\n",
    "\n",
    "def train_epoch(model, dloader, optim, lr_scheduler, device):\n",
    "    cum_loss =  0.0\n",
    "\n",
    "    for x, y in tqdm(dloader):\n",
    "        lr_scheduler.adjust_lr()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred.transpose(1, 2), y)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        cum_loss += loss.item() * dloader.batch_size\n",
    "\n",
    "        for param_group in optim.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "    \n",
    "    cum_loss /= (len(dloader) * batch_size)\n",
    "\n",
    "    return cum_loss\n",
    "\n",
    "def test_epoch(model, dloader, device):\n",
    "    cum_loss =  0.0\n",
    "\n",
    "    for x, y in dloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred.transpose(1, 2), y)\n",
    "\n",
    "        cum_loss += loss.item() * dloader.batch_size\n",
    "    \n",
    "    cum_loss /= (len(dloader) * batch_size)\n",
    "\n",
    "    return cum_loss\n",
    "\n",
    "torch.manual_seed(2131)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "tokenizer_path = \"tokenizer.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "pad_token = tokenizer.token_to_id(\"<pad>\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "collate_fn = partial(collate_batch_fn, pad_token=pad_token)\n",
    "train_dloader = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dloader = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dloader = DataLoader(test_ds, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = NanoGPT()\n",
    "\n",
    "# Gradient clipping\n",
    "# for p in model.parameters():\n",
    "#     p.register_hook(lambda grad: )\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optim = AdamW(model.parameters())\n",
    "total_steps = len(train_dloader) * num_epochs\n",
    "warmup_steps = total_steps * 0.05\n",
    "# lr_scheduler = LRScheduler(optim, warmup_steps=warmup_steps)\n",
    "lr_scheduler = GPT2LRScheduler(optim, warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "model = model.to(device)\n",
    "print(summary(model))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_dloader, optim, lr_scheduler, device)\n",
    "    val_loss = test_epoch(model, val_dloader, device)\n",
    "    perplexity = np.exp(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}, Val Perplexity: {perplexity}\")\n",
    "\n",
    "test_loss = test_epoch(model, test_dloader, device)\n",
    "perplexity = np.exp(test_loss)\n",
    "print(f\"Test Loss: {test_loss}, Test Perplexity: {perplexity}\")\n",
    "torch.save(model.module.state_dict(), \"weights.pt\")\n",
    "# torch.save(model.state_dict(), \"weights.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6173f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let him:\n",
      "Ay, I am going with me,\n",
      "That I do,\n",
      "My lord.\n",
      "My lord, my lord.\n",
      "You must:\n",
      "So I'll not.\n",
      "DUKE VINCENTIO:\n",
      "DUKE VINCENTIO:\n",
      "DUKE VINCENTIO:\n",
      "I do, I have done, I will.\n",
      "I know you?\n",
      "And I know's the people, my lord, sir, and I had I had been\n",
      "You will.\n",
      "And you are:\n",
      "ISABELLA:\n",
      "DUKE VINCENTIO:\n",
      "LUCIO:\n",
      "LUCIO:\n",
      "To your husband\n",
      "LUCIO:\n",
      "DUKE VINCENTIO:\n",
      "I'll do not a very heart.\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPT()\n",
    "model.load_state_dict(torch.load(\"weights.pt\", weights_only=True))\n",
    "out = model.run_inference(\"Let\")\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
