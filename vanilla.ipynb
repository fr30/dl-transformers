{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de2e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 48 49\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import TinyShakespeareDataset\n",
    "    \n",
    "\n",
    "train_ds = TinyShakespeareDataset('train')\n",
    "val_ds = TinyShakespeareDataset('val')\n",
    "test_ds = TinyShakespeareDataset('test')\n",
    "\n",
    "print(len(train_ds), len(val_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d5868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "class FFLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        dff = emb_dim * 8 // 3\n",
    "        self.W1 = nn.Linear(emb_dim, dff)\n",
    "        self.W2 = nn.Linear(dff, emb_dim)\n",
    "        self.W3 = nn.Linear(emb_dim, dff)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.silu(self.W1(x))\n",
    "        y = y * self.W3(x)\n",
    "        y = self.W2(y)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def silu(x):\n",
    "        return x * F.sigmoid(x)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_size, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(emb_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_dtype = x.dtype\n",
    "        rms = torch.sqrt(torch.mean(x.pow(2)) + self.eps)\n",
    "        x = (x / rms) * self.g\n",
    "        return x.to(in_dtype)\n",
    "\n",
    "class CausalSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=512, max_seq_len=1050, n_heads=8, p_dropout=0.1):\n",
    "        super().__init__()\n",
    "        # self.Wq = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        # self.Wk = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        # self.Wv = nn.Linear(input_dim, emb_dim, bias=True)\n",
    "        self.W = nn.Linear(input_dim, emb_dim * 3, bias=True)\n",
    "        self.Wo = nn.Linear(emb_dim, emb_dim)\n",
    "        self.ff = FFLayer(emb_dim)\n",
    "        self.ln_mha = RMSNorm(emb_dim)\n",
    "        self.ln_ff = RMSNorm(emb_dim)        \n",
    "        self.scale = 1 / np.sqrt(emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        attn_mask = torch.tril(torch.ones(max_seq_len, max_seq_len)) == 0\n",
    "        self.register_buffer(\"_causal_mask\", attn_mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.mha(x)\n",
    "        y_prenorm = y + x\n",
    "\n",
    "        y = self.mlp(y_prenorm)\n",
    "        y = y_prenorm + y\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def mlp(self, x):\n",
    "        y = self.ln_ff(x)\n",
    "        y = self.ff(y)\n",
    "        y = F.dropout(y, p=self.p_dropout)\n",
    "    \n",
    "        return y\n",
    "    \n",
    "    def mha(self, x):\n",
    "        b, t, e = x.shape\n",
    "        s = e // self.n_heads\n",
    "        \n",
    "        # x = self.ln_mha(x)\n",
    "        # q, k, v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        # q = q.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "        # k = k.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "        # v = v.view(b, t, self.n_heads, s).transpose(1, 2)\n",
    "\n",
    "        x = self.ln_mha(x)\n",
    "        qkv = self.W(x)\n",
    "        qkv = qkv.view(b, t, self.n_heads, 3 * s)\n",
    "        qkv = qkv.transpose(1, 2)\n",
    "        q, k, v = qkv.split(s, dim=-1)\n",
    "\n",
    "        attn = q @ k.transpose(-1, -2) * self.scale\n",
    "        attn = attn.masked_fill(self._causal_mask[:t, :t], -torch.inf)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = F.dropout(attn, p=self.p_dropout)\n",
    "        y = attn @ v\n",
    "        y = y.transpose(-2, -3).reshape(b, t, e)\n",
    "        y = self.Wo(y)\n",
    "        y = F.dropout(y, p=self.p_dropout)\n",
    "\n",
    "        return y\n",
    "\n",
    "        \n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size=8124, \n",
    "            emb_dim=512, \n",
    "            attn_blocks=4, \n",
    "            max_seq_len=1024, \n",
    "            n_heads=16, \n",
    "            p_dropout=0.1, \n",
    "            tokenizer_path=\"tokenizer.json\"\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.attn = nn.Sequential(\n",
    "            *[CausalSelfAttentionBlock(emb_dim, emb_dim, max_seq_len, n_heads, p_dropout) for _ in range(attn_blocks)]\n",
    "        )\n",
    "        self.mlp = nn.Linear(emb_dim, vocab_size)\n",
    "        self.layer_norm = RMSNorm(emb_dim)\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        self.register_buffer(\"_device_tracker\", torch.empty(0))\n",
    "        pe = self._compute_pe(max_seq_len, emb_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device_tracker.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, t = x.shape\n",
    "        x = self.emb(x)\n",
    "        x = x + self.pe[:, :t, :]\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.attn(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def run_inference(self, text_input, tau=1.0, k=10):\n",
    "        x = self.tokenizer.encode(text_input).ids[:-1]\n",
    "        eos_id = self.tokenizer.token_to_id(\"<eos>\")\n",
    "        next_token = -1\n",
    "        cur_iter = 0\n",
    "        max_iter = 128\n",
    "\n",
    "        while (next_token != eos_id and cur_iter < max_iter):\n",
    "            x_tensor = torch.tensor(x).to(self.device)\n",
    "            logits = self.forward(x_tensor.unsqueeze(0))\n",
    "            q = F.softmax(logits / tau, dim=-1)[:, -1]\n",
    "            topk = q.topk(k=k)\n",
    "            next_token_index = topk.values.multinomial(1).item()\n",
    "            next_token = topk.indices[0, next_token_index]\n",
    "            x += [next_token]\n",
    "            cur_iter += 1\n",
    "        \n",
    "        return self.tokenizer.decode(x)\n",
    "        \n",
    "    def _compute_pe(self, max_seq_len, emb_dim):\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-np.log(10000.0) / emb_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) \n",
    "\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff833460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class GPT2LRScheduler:\n",
    "    def __init__(self, optim, max_lr=1e-3, min_lr=3e-5, warmup_steps=2000, total_steps=200000):\n",
    "        self.optim = optim\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.warmup_steps = max(1, int(warmup_steps))\n",
    "        self.total_steps = max(self.warmup_steps + 1, int(total_steps))\n",
    "        self.step = 0\n",
    "\n",
    "    def adjust_lr(self):\n",
    "        self.step += 1\n",
    "        s = self.step\n",
    "\n",
    "        if s <= self.warmup_steps:\n",
    "            # linear warmup: 0 -> max_lr\n",
    "            lr = self.max_lr * (s / self.warmup_steps)\n",
    "        else:\n",
    "            # cosine decay: max_lr -> min_lr\n",
    "            progress = (s - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            progress = min(max(progress, 0.0), 1.0)\n",
    "            cosine = 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * cosine\n",
    "\n",
    "        for pg in self.optim.param_groups:\n",
    "            pg[\"lr\"] = lr\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db60fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "DataParallel                                  --\n",
      "├─NanoGPT: 1-1                                --\n",
      "│    └─Embedding: 2-1                         4,159,488\n",
      "│    └─Sequential: 2-2                        --\n",
      "│    │    └─CausalSelfAttentionBlock: 3-1     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-2     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-3     3,151,530\n",
      "│    │    └─CausalSelfAttentionBlock: 3-4     3,151,530\n",
      "│    └─Linear: 2-3                            4,167,612\n",
      "│    └─RMSNorm: 2-4                           512\n",
      "======================================================================\n",
      "Total params: 20,933,732\n",
      "Trainable params: 20,933,732\n",
      "Non-trainable params: 0\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:11<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 5.356269809434998, Val Loss: 4.5259759823481245, Val Perplexity: 92.38604899522646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:10<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 4.513279946345203, Val Loss: 4.277840216954549, Val Perplexity: 72.08458469721414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:10<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 4.173064083423254, Val Loss: 4.1404792467753095, Val Perplexity: 62.832926712210444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:10<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 3.948290964342513, Val Loss: 4.091726819674174, Val Perplexity: 59.84314084514037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:10<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 3.836419330452973, Val Loss: 4.08082898457845, Val Perplexity: 59.19452086072973\n",
      "Test Loss: 4.838550158909389, Test Perplexity: 126.28612412482508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# def clip_gradient(grad):\n",
    "#     max_grad_norm = \n",
    "\n",
    "def collate_batch_fn(args, pad_token):\n",
    "    x = [t[0] for t in args]\n",
    "    y = [t[1] for t in args]\n",
    "    xlen = len(x)\n",
    "    padded_xy = pad_sequence(x + y, padding_value=pad_token, batch_first=True)\n",
    "    padded_x = padded_xy[:xlen]\n",
    "    padded_y = padded_xy[xlen:]\n",
    "\n",
    "    return padded_x, padded_y\n",
    "\n",
    "lrs = []\n",
    "\n",
    "def train_epoch(model, dloader, optim, lr_scheduler, device):\n",
    "    cum_loss =  0.0\n",
    "\n",
    "    for x, y in tqdm(dloader):\n",
    "        lr_scheduler.adjust_lr()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred.transpose(1, 2), y)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        cum_loss += loss.item() * dloader.batch_size\n",
    "\n",
    "        for param_group in optim.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "    \n",
    "    cum_loss /= (len(dloader) * batch_size)\n",
    "\n",
    "    return cum_loss\n",
    "\n",
    "def test_epoch(model, dloader, device):\n",
    "    cum_loss =  0.0\n",
    "\n",
    "    for x, y in dloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            loss = F.cross_entropy(y_pred.transpose(1, 2), y)\n",
    "\n",
    "        cum_loss += loss.item() * dloader.batch_size\n",
    "    \n",
    "    cum_loss /= (len(dloader) * batch_size)\n",
    "\n",
    "    return cum_loss\n",
    "\n",
    "torch.manual_seed(2131)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "tokenizer_path = \"tokenizer.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "pad_token = tokenizer.token_to_id(\"<pad>\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "collate_fn = partial(collate_batch_fn, pad_token=pad_token)\n",
    "\n",
    "train_dloader = DataLoader(train_ds, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dloader = DataLoader(val_ds, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dloader = DataLoader(test_ds, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = NanoGPT()\n",
    "\n",
    "# Gradient clipping\n",
    "# for p in model.parameters():\n",
    "#     p.register_hook(lambda grad: )\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "optim = AdamW(model.parameters())\n",
    "total_steps = len(train_dloader) * num_epochs\n",
    "warmup_steps = total_steps * 0.05\n",
    "# lr_scheduler = LRScheduler(optim, warmup_steps=warmup_steps)\n",
    "lr_scheduler = GPT2LRScheduler(optim, warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "model = model.to(device)\n",
    "print(summary(model))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_dloader, optim, lr_scheduler, device)\n",
    "    val_loss = test_epoch(model, val_dloader, device)\n",
    "    perplexity = np.exp(val_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}, Val Perplexity: {perplexity}\")\n",
    "\n",
    "test_loss = test_epoch(model, test_dloader, device)\n",
    "perplexity = np.exp(test_loss)\n",
    "print(f\"Test Loss: {test_loss}, Test Perplexity: {perplexity}\")\n",
    "torch.save(model.module.state_dict(), \"weights.pt\")\n",
    "# torch.save(model.state_dict(), \"weights.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6173f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let him\n",
      "The gods.\n",
      "I would I am.\n",
      "I think, my lords,\n",
      "I would not;\n",
      "I do it so, to be the crown the world\n",
      "He's no more to the queen's name,\n",
      "What, and all the world'st thou shalt not a man:\n",
      "What is no, I will be not in the prince:\n",
      "I will\n",
      "What's to my son\n",
      "To keep your grace\n",
      "I will I would not to-morrow:\n",
      "The gods\n",
      "That I do this,\n",
      "The gods! the king,\n",
      "KING HENRY VI\n",
      "To make me with my brother is the field\n"
     ]
    }
   ],
   "source": [
    "model = NanoGPT()\n",
    "model.load_state_dict(torch.load(\"weights.pt\", weights_only=True))\n",
    "out = model.run_inference(\"Let\")\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
